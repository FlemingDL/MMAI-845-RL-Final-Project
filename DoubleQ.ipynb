{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DoubleQ.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEXIVM65oapM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv3MDxNv8dHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37R98CM3uNLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/FlemingDL/gym_fleming.git\n",
        "\n",
        "!pip install -e gym_fleming"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGWbBY4Aup1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "from contextlib import closing\n",
        "from six import StringIO\n",
        "from gym import utils\n",
        "from gym.envs.toy_text import discrete\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwzAIZwt9VhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import gym_fleming\n",
        "from gym import wrappers\n",
        "\n",
        "env = gym.make('taxi_fleming-v0')\n",
        "env = gym.wrappers.Monitor(env, \"./drive/My Drive/double_Q\",  video_callable=lambda episode_id: episode_id%500==0, force=True)\n",
        "#  env = gym.wrappers.Monitor(env, directory, video_callable=lambda episode_id: episode_id%10==0)\n",
        "n_state = env.observation_space.n\n",
        "print(n_state)\n",
        "\n",
        "n_action = env.action_space.n\n",
        "print(n_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c888oFKpHpe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.render()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwYg-nUOJ3-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# class TaxiEnv(discrete.DiscreteEnv):\n",
        "#     \"\"\"\n",
        "#     The Taxi Problem\n",
        "#     from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"\n",
        "#     by Tom Dietterich\n",
        "#     Description:\n",
        "#     There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
        "#     Observations: \n",
        "#     There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations. \n",
        "    \n",
        "#     Passenger locations:\n",
        "#     - 0: R(ed)\n",
        "#     - 1: G(reen)\n",
        "#     - 2: Y(ellow)\n",
        "#     - 3: B(lue)\n",
        "#     - 4: in taxi\n",
        "    \n",
        "#     Destinations:\n",
        "#     - 0: R(ed)\n",
        "#     - 1: G(reen)\n",
        "#     - 2: Y(ellow)\n",
        "#     - 3: B(lue)\n",
        "        \n",
        "#     Actions:\n",
        "#     There are 6 discrete deterministic actions:\n",
        "#     - 0: move south\n",
        "#     - 1: move north\n",
        "#     - 2: move east \n",
        "#     - 3: move west \n",
        "#     - 4: pickup passenger\n",
        "#     - 5: dropoff passenger\n",
        "    \n",
        "#     Rewards: \n",
        "#     There is a reward of -1 for each action and an additional reward of +20 for delivering the passenger. There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally.\n",
        "    \n",
        "#     Rendering:\n",
        "#     - blue: passenger\n",
        "#     - magenta: destination\n",
        "#     - yellow: empty taxi\n",
        "#     - green: full taxi\n",
        "#     - other letters (R, G, Y and B): locations for passengers and destinations\n",
        "    \n",
        "#     state space is represented by:\n",
        "#         (taxi_row, taxi_col, passenger_location, destination)\n",
        "#     \"\"\"\n",
        "#     metadata = {'render.modes': ['human', 'ansi']}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsAvA0KEKdbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ">>> def double_q_learning(env, gamma, n_episode, alpha):\n",
        "...     \"\"\"\n",
        "...     Obtain the optimal policy with off-policy double \n",
        "        Q-learning method\n",
        "...     @param env: OpenAI Gym environment\n",
        "...     @param gamma: discount factor\n",
        "...     @param n_episode: number of episodes\n",
        "...     @return: the optimal Q-function, and the optimal policy\n",
        "...     \"\"\"\n",
        "...     n_action = env.action_space.n\n",
        "...     n_state = env.observation_space.n\n",
        "...     Q1 = torch.zeros(n_state, n_action)\n",
        "...     Q2 = torch.zeros(n_state, n_action)\n",
        "...     frames =[]\n",
        "...     for episode in range(n_episode):\n",
        "...         state = env.reset()\n",
        "...         is_done = False\n",
        "...         while not is_done:\n",
        "# ...             env.render()\n",
        "# ...             env.close()\n",
        "...             action = epsilon_greedy_policy(state, Q1 + Q2)\n",
        "...             next_state, reward, is_done, info = env.step(action)\n",
        "...             if (torch.rand(1).item() < 0.5):\n",
        "...                 best_next_action = torch.argmax(Q1[next_state])\n",
        "...                 td_delta = reward + gamma * Q2[next_state][best_next_action]  - Q1[state][action]\n",
        "...                 Q1[state][action] += alpha * td_delta\n",
        "...             else:\n",
        "...                 best_next_action = torch.argmax(Q2[next_state])\n",
        "...                 td_delta = reward + gamma * Q1[next_state][best_next_action] - Q2[state][action]\n",
        "...                 Q2[state][action] += alpha * td_delta\n",
        "...             length_episode[episode] += 1\n",
        "...             total_reward_episode[episode] += reward\n",
        "# ...             frames.append({'frame': env.render(mode='ansi'),'state': state,'action': action,'reward': reward})\n",
        "...             if is_done:\n",
        "...                 break\n",
        "...             state = next_state\n",
        "# ...     env.close()\n",
        "...     policy = {}\n",
        "...     Q = Q1 + Q2\n",
        "\n",
        "\n",
        "...     for state in range(n_state):\n",
        "...         policy[state] = torch.argmax(Q[state]).item()\n",
        "...     return Q, policy#,frames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDirCtF8SwUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ">>> def gen_epsilon_greedy_policy(n_action, epsilon):\n",
        "        def policy_function(state, Q):\n",
        "            probs = torch.ones(n_action) * epsilon / n_action\n",
        "            best_action = torch.argmax(Q[state]).item()\n",
        "            probs[best_action] += 1.0 - epsilon\n",
        "            action = torch.multinomial(probs, 1).item()\n",
        "            return action\n",
        "        return policy_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAMy0klpCPan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQfSMeJh399d",
        "colab_type": "text"
      },
      "source": [
        "Gamma = 1, Alpha = 0.4, Epsilon = 0.1\n",
        "\n",
        "\n",
        "141\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IVvwFHENJft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ">>> n_episode = 2000\n",
        ">>> length_episode = [0] * n_episode\n",
        ">>> total_reward_episode = [0] * n_episode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoSr3n4XKde7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ">>> gamma = 1\n",
        ">>> alpha = 0.4\n",
        ">>> epsilon = 0.1\n",
        ">>> epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space.n, epsilon)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TQHcFPCHpms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ">>> optimal_Q, optimal_policy = double_q_learning(env, gamma, n_episode, alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nejPtajhB7Cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print_frames(frames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbQc16_gqNru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length_episode_141 = length_episode\n",
        "total_reward_episode_141 = total_reward_episode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gELDHY7l6Ycl",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.plot(length_episode)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Length')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8Sc2FK7v6Ycq",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.plot(total_reward_episode)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total reward')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c-X3tMaGX-b",
        "colab_type": "text"
      },
      "source": [
        "BOKEHHHHHHHHHHHHH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtCCv9QC258e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bokeh.layouts import gridplot\n",
        "from bokeh.plotting import figure, output_file, show\n",
        "import numpy as np\n",
        "\n",
        "from bokeh.io import show\n",
        "from bokeh.layouts import column\n",
        "from bokeh.models import ColumnDataSource, RangeTool\n",
        "from bokeh.plotting import figure"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5zWoh9L9sMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y=np.array(length_episode)\n",
        "Y1=np.array(total_reward_episode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJaJ_HJixd61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TOOLS = \"pan,wheel_zoom,box_zoom,reset,save,box_select\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqroMqO6xd64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p3 = figure(title=\"Episode length over time New Map\", tools=TOOLS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gfp5PRzxd66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "##p3.circle(range(0, 1000), Y, legend_label=\"\")\n",
        "p3.line(range(0, 2000), Y, legend_label=\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr5MkAYNxd69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p3.legend.title = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9dsnR88_y4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_file(\"lenght.html\", title=\"Episode length over time New Map\")\n",
        "\n",
        "show(gridplot([p3], ncols=2, plot_width=1000, plot_height=400))  # open a browser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ARfqvq7xd6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p4 = figure(title=\"Episode reward over time New Map\", tools=TOOLS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyepGTt3xd7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##p4.circle(range(0, 1000),Y1, legend_label=\"\",color=\"orange\")\n",
        "p4.line(range(0, 2000),Y1, legend_label=\"\",color=\"orange\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UAvYAJKxd7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p4.legend.title = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJkcGBjTxd7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_file(\"reward.html\", title=\"Episode length over time New Map\")\n",
        "\n",
        "show(gridplot([p4], ncols=2, plot_width=1000, plot_height=400))  # open a browser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRPe0EHu9ARr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLebJnd39AUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXJdqNN-9AaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-1JzCF09Axj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoSpaJtC9Apg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvlg5cNb9AmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYkA1tliGT3L",
        "colab_type": "text"
      },
      "source": [
        "TUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujIkFKMs4fIA",
        "colab_type": "text"
      },
      "source": [
        "Gamma = 1, Alpha = 0.4, Epsilon = 0.4\n",
        "\n",
        "\n",
        "144\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i_OYThAcNkFh",
        "colab": {}
      },
      "source": [
        ">>> n_episode = 1000\n",
        ">>> length_episode = [0] * n_episode\n",
        ">>> total_reward_episode = [0] * n_episode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4rgvV0FNkFm",
        "colab": {}
      },
      "source": [
        ">>> gamma = 1\n",
        ">>> alpha = 0.4\n",
        ">>> epsilon = 0.4\n",
        ">>> epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space.n, epsilon)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aodqFgkBNkFo",
        "colab": {}
      },
      "source": [
        ">>> optimal_Q, optimal_policy = double_q_learning(env, gamma, n_episode, alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jOuT-DSVNkFr",
        "colab": {}
      },
      "source": [
        "length_episode_144 = length_episode\n",
        "total_reward_episode_144 = total_reward_episode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WJUTXJ8FNkFt",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.plot(length_episode)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Length')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UeCGQDQENkFw",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.plot(total_reward_episode)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total reward')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4l53w8GNfI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrqbw26V47TO",
        "colab_type": "text"
      },
      "source": [
        "Gamma = 1, Alpha = 0.1, Epsilon = 0.4\n",
        "\n",
        "\n",
        "114\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Artrco0aNyVs",
        "colab": {}
      },
      "source": [
        ">>> n_episode = 1000\n",
        ">>> length_episode = [0] * n_episode\n",
        ">>> total_reward_episode = [0] * n_episode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UXjkzozDNyVy",
        "colab": {}
      },
      "source": [
        ">>> gamma = 1\n",
        ">>> alpha = 0.1\n",
        ">>> epsilon = 0.4\n",
        ">>> epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space.n, epsilon)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AG7rcnd6NyV0",
        "colab": {}
      },
      "source": [
        ">>> optimal_Q, optimal_policy = double_q_learning(env, gamma, n_episode, alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L_ZezbtjNyV3",
        "colab": {}
      },
      "source": [
        "length_episode_114 = length_episode\n",
        "total_reward_episode_114 = total_reward_episode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lDNrakq4NyV6",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.plot(length_episode)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Length')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "94NDFj6iNyV8",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.plot(total_reward_episode)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total reward')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-FiKQejNfLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DylTicmw59CZ",
        "colab_type": "text"
      },
      "source": [
        "Gamma = 1, Alpha = 0.1, Epsilon = 0.1\n",
        "\n",
        "\n",
        "111"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sJJ1IL9xNy_H",
        "colab": {}
      },
      "source": [
        ">>> n_episode = 1000\n",
        ">>> length_episode = [0] * n_episode\n",
        ">>> total_reward_episode = [0] * n_episode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uiLFNzqoNy_K",
        "colab": {}
      },
      "source": [
        ">>> gamma = 1\n",
        ">>> alpha = 0.1\n",
        ">>> epsilon = 0.1\n",
        ">>> epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space.n, epsilon)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LCNV3a97Ny_N",
        "colab": {}
      },
      "source": [
        ">>> optimal_Q, optimal_policy = double_q_learning(env, gamma, n_episode, alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VrXBnAh0Ny_Q",
        "colab": {}
      },
      "source": [
        "length_episode_111 = length_episode\n",
        "total_reward_episode_111 = total_reward_episode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q6QoPNZoNy_S",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.plot(length_episode)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Length')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7_VLwPeRNy_V",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.plot(total_reward_episode)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total reward')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G09_uuLrVhiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdm1-XHkOJL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrTGdOnU6HiB",
        "colab_type": "text"
      },
      "source": [
        "Length vs episode plot for :\n",
        "141,144,111,114\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPttsoG_OJWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(length_episode_141, color='#1E1E1E')\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(length_episode_144, color='#1E1E1E')\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(length_episode_111, color='#1E1E1E')\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(length_episode_114, color='#1E1E1E')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Length')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZzTOSL1OJcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X_Js14-6Pjs",
        "colab_type": "text"
      },
      "source": [
        "Total Reward vs episode plot for :\n",
        "141,144,111,114"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XvfNfWIOJVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(total_reward_episode_141, color='#1E1E1E')\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(total_reward_episode_144, color='#1E1E1E')\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(total_reward_episode_111, color='#1E1E1E')\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(total_reward_episode_114, color='#1E1E1E')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Length')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4z4YrmoOJR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}